import numpy as npfrom sklearn.metrics import roc_curvefrom sklearn.preprocessing import LabelEncoderfrom keras.utils import np_utilsfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.linear_model import LogisticRegressionimport pldadef load_ivector(filename):    utt = np.loadtxt(filename, dtype=np.str, delimiter=',', skiprows=1, usecols=[0])    ivector = np.loadtxt(filename, dtype=np.float, delimiter=',', skiprows=1, usecols=range(1, 601))    spk_id = []    for iter in range(len(utt)):        spk_id = np.append(spk_id, utt[iter].split('_')[0])    return spk_id, utt, ivectordef length_norm(mat):    """    length normalization (l2 norm)    input: mat = [utterances X vector dimension] ex) (float) 8631 X 600    """    norm_mat = []    for line in mat:        temp = line/np.math.sqrt(sum(np.power(line, 2)))        norm_mat.append(temp)    norm_mat = np.array(norm_mat)    return norm_matdef make_spkvec(mat, spk_label):    """    calculating speaker mean vector    input: mat = [utterances X vector dimension] ex) (float) 8631 X 600           spk_label = string vector ex) ['abce','cdgd']        for iter in range(len(spk_label)):            spk_label[iter] = spk_label[iter].split('_')[0]    """    spk_label, spk_index = np.unique(spk_label, return_inverse=True)    spk_mean = []    mat = np.array(mat)    # calculating speaker mean i-vector    for i, spk in enumerate(spk_label):        spk_mean.append(np.mean(mat[np.nonzero(spk_index == i)], axis=0))    spk_mean = length_norm(spk_mean)    return spk_mean, spk_labeldef calculate_EER(trials, scores):    # calculating EER of Top-S detector    # input: trials = boolean(or int) vector, 1: positive(blacklist) 0: negative(background)    #        scores = float vector    # Calculating EER    fpr, tpr, threshold = roc_curve(trials, scores, pos_label=1)    fnr = 1 - tpr    EER_threshold = threshold[np.argmin(abs(fnr - fpr))]    # print EER_threshold    EER_fpr = fpr[np.nanargmin(np.absolute((fnr - fpr)))]    EER_fnr = fnr[np.nanargmin(np.absolute((fnr - fpr)))]    EER = 0.5 * (EER_fpr + EER_fnr)    print("Top S detector EER is %0.2f%%" % (EER * 100))    return EERdef get_trials_label_with_confusion(identified_label, groundtruth_label, dict4spk, is_trial):    # determine if the test utterance would make confusion error    # input: identified_label = string vector, identified result of test utterance among multi-target from the detection system    #        groundtruth_label = string vector, ground truth speaker labels of test utterances    #        dict4spk = dictionary, convert label to target set, ex) train2dev convert train id to dev id    trials = np.zeros(len(identified_label))    for iter in range(0, len(groundtruth_label)):        enroll = identified_label[iter].split('_')[0]        test = groundtruth_label[iter].split('_')[0]        if is_trial[iter]:            if enroll == dict4spk[test]:                trials[iter] = 1  # for Target trial (blacklist speaker)            else:                trials[iter] = -1  # for Target trial (blacklist speaker), but fail on blacklist classifier        else:            trials[iter] = 0  # for non-target (non-blacklist speaker)    return trialsdef calculate_EER_with_confusion(scores, trials):    # calculating EER of Top-1 detector    # input: trials = boolean(or int) vector, 1: postive(blacklist) 0: negative(background) -1: confusion(blacklist)    #        scores = float vector    # exclude confusion error (trials==-1)    scores_wo_confusion = scores[np.nonzero(trials != -1)[0]]    trials_wo_confusion = trials[np.nonzero(trials != -1)[0]]    # dev_trials contain labels of target. (target=1, non-target=0)    fpr, tpr, threshold = roc_curve(trials_wo_confusion, scores_wo_confusion, pos_label=1, drop_intermediate=False)    fnr = 1 - tpr    EER_threshold = threshold[np.argmin(abs(fnr - fpr))]    # EER withouth confusion error    EER = fpr[np.argmin(np.absolute((fnr - fpr)))]    # Add confusion error to false negative rate(Miss rate)    total_negative = len(np.nonzero(np.array(trials_wo_confusion) == 0)[0])    total_positive = len(np.nonzero(np.array(trials_wo_confusion) == 1)[0])    fp = fpr * np.float(total_negative)    fn = fnr * np.float(total_positive)    fn += len(np.nonzero(trials == -1)[0])    total_positive += len(np.nonzero(trials == -1)[0])    fpr = fp / total_negative    fnr = fn / total_positive    # EER with confusion Error    EER_threshold = threshold[np.argmin(abs(fnr - fpr))]    EER_fpr = fpr[np.argmin(np.absolute((fnr - fpr)))]    EER_fnr = fnr[np.argmin(np.absolute((fnr - fpr)))]    EER = 0.5 * (EER_fpr + EER_fnr)    print("Top 1 detector EER is %0.2f%% (Total confusion error is %d)" % ((EER * 100), len(np.nonzero(trials == -1)[0])))    return EERif __name__ == '__main__':    # making dictionary to find blacklist pair between train and test dataset    bl_match = np.loadtxt('data/bl_matching.csv', dtype=np.str)    dev2train = {}    dev2id = {}    train2dev = {}    train2id = {}    test2train = {}    train2test = {}    for iter, line in enumerate(bl_match):        line_s = line.split(',')        dev2train[line_s[1].split('_')[-1]] = line_s[3].split('_')[-1]        dev2id[line_s[1].split('_')[-1]] = line_s[0].split('_')[-1]        train2dev[line_s[3].split('_')[-1]] = line_s[1].split('_')[-1]        train2id[line_s[3].split('_')[-1]] = line_s[0].split('_')[-1]        test2train[line_s[2].split('_')[-1]] = line_s[3].split('_')[-1]        train2test[line_s[3].split('_')[-1]] = line_s[2].split('_')[-1]    # Loading i-vector    print('Loading i-vectors')    trn_bl_id, trn_bl_utt, trn_bl_ivector = load_ivector('data/trn_blacklist.csv')    trn_bg_id, trn_bg_utt, trn_bg_ivector = load_ivector('data/trn_background.csv')    dev_bl_id, dev_bl_utt, dev_bl_ivector = load_ivector('data/dev_blacklist.csv')    dev_bg_id, dev_bg_utt, dev_bg_ivector = load_ivector('data/dev_background.csv')    tst_id, tst_utt, tst_ivector = load_ivector('data/tst_evaluation.csv')    # convert labels of development set    dev_trials_label = np.append(dev_bl_id, dev_bg_id)    dev_bl_id = [dev2train[dev_id] for dev_id in dev_bl_id]    dev_ivector = np.append(dev_bl_ivector, dev_bg_ivector, axis=0)    trn_ivector = np.append(trn_bl_ivector, trn_bg_ivector, axis=0)    # encode labels    le = LabelEncoder()    trn_id_enc = le.fit_transform(trn_bl_id)    num_bl_spk = len(np.unique(trn_id_enc))    trn_id_cat = np_utils.to_categorical(trn_id_enc, num_classes=num_bl_spk)    dev_id_cat = np_utils.to_categorical(le.transform(dev_bl_id), num_classes=num_bl_spk)    # length normalization    print('normalizing lengths')    trn_bl_ivector_ln = length_norm(trn_bl_ivector)    dev_bl_ivector_ln = length_norm(dev_bl_ivector)    trn_bg_ivector_ln = length_norm(trn_bg_ivector)    dev_ivector_ln = length_norm(dev_ivector)    trn_ivector_ln = length_norm(trn_ivector)    tst_ivector_ln = length_norm(tst_ivector)    # Evaluating Cosine Similarity with M-Norm    print('evaluating cosing similarity with M-Norm')    spk_mean, spk_mean_label = make_spkvec(trn_bl_ivector_ln, trn_bl_id)    dev_scores = spk_mean.dot(dev_ivector_ln.transpose())    blscores = spk_mean.dot(trn_bl_ivector_ln.transpose())    mnorm_mu_bl = np.mean(blscores, axis=1)    mnorm_std_bl = np.std(blscores, axis=1)    for iter in range(np.shape(dev_scores)[1]):        dev_scores[:, iter] = (dev_scores[:, iter]-mnorm_mu_bl)# / mnorm_std_bl    dev_scores_bl = np.max(dev_scores, axis=0)    # load dev and test set information    dev_trials = np.append(np.ones([len(dev_bl_id), 1]), np.zeros([len(dev_bg_id), 1]))    filename = 'data/tst_evaluation_keys.csv'    tst_info = np.loadtxt(filename, dtype=np.str, delimiter=',', skiprows=1, usecols=range(0, 3))    tst_trials = []    tst_trials_label = []    tst_ground_truth = []    for iter in range(len(tst_info)):        tst_trials_label.extend([tst_info[iter, 0]])        if tst_info[iter, 1] == 'background':            tst_trials = np.append(tst_trials, 0)        else:            tst_trials = np.append(tst_trials, 1)    # apply LDA for PLDA    print('Applying Linear Discriminant Analysis')    clf_deep = LinearDiscriminantAnalysis(n_components=600)    clf_deep.fit(trn_bl_ivector, trn_id_enc)    trn_bl_ivector_lda = clf_deep.transform(trn_bl_ivector)    tst_ivector_lda = clf_deep.transform(tst_ivector)    dev_ivector_lda = clf_deep.transform(dev_ivector)    trn_bl_ivector_lda = length_norm(trn_bl_ivector_lda)    tst_ivector_lda = length_norm(tst_ivector_lda)    dev_ivector_lda = length_norm(dev_ivector_lda)    spk_mean_lda, spk_mean_label = make_spkvec(trn_bl_ivector_lda, trn_bl_id)    # evalute plda    print('Training plda model')    numiter = 20    vdim = 600    udim = 600    plda_model_path = 'plda_model_' + str(numiter) + '_' + str(vdim) + '_' + str(udim) + '.npy'    plda_model = plda.fit_plda_model(trn_bl_ivector_lda, trn_id_cat, numiter=numiter, vdim=vdim, udim=udim, rcond=1e-3)    scores_plda = plda.get_plda_scores(plda_model, spk_mean_lda, tst_ivector_lda)    dev_scores_plda = plda.get_plda_scores(plda_model, spk_mean_lda, dev_ivector_lda)    # global min-max normalization    dev_min_plda = np.min(dev_scores_plda)    dev_max_plda = np.max(dev_scores_plda)    dev_scores_plda = (dev_scores_plda - dev_min_plda) / (dev_max_plda - dev_min_plda)    scores_plda = (scores_plda - dev_min_plda) / (dev_max_plda - dev_min_plda)    # top-1 detector EER    dev_identified_label = spk_mean_label[(np.argmax(dev_scores_plda, axis=0))]    dev_trials_confusion = get_trials_label_with_confusion(dev_identified_label, dev_trials_label, dev2train,                                                           dev_trials)    # apply LDA for blacklist speakers    print('Applying Linear Discriminant Analysis')    clf_deep = LinearDiscriminantAnalysis(n_components=200)    clf_deep.fit(trn_bl_ivector, trn_id_enc)    trn_bl_ivector_lda = clf_deep.transform(trn_bl_ivector)    dev_ivector_lda = clf_deep.transform(dev_ivector)    trn_bl_ivector_lda = length_norm(trn_bl_ivector_lda)    dev_ivector_lda = length_norm(dev_ivector_lda)    spk_mean_lda, spk_mean_label = make_spkvec(trn_bl_ivector_lda, trn_bl_id)    dev_scores_lda = spk_mean_lda.dot(dev_ivector_lda.transpose())    # fuse scores with Logistic Regression    clf = LogisticRegression(class_weight='balanced').fit(np.stack([np.max(dev_scores, axis=0), np.mean(dev_scores, axis=0),                                                                    np.max(dev_scores_lda, axis=0), np.mean(dev_scores_lda, axis=0),                                                                    np.max(dev_scores_plda, axis=0), np.mean(dev_scores_plda, axis=0)], axis=1), dev_trials)    dev_scores_fused = clf.predict_proba(np.stack([np.max(dev_scores, axis=0), np.mean(dev_scores, axis=0),                                                   np.max(dev_scores_lda, axis=0), np.mean(dev_scores_lda, axis=0),                                                   np.max(dev_scores_lda, axis=0), np.mean(dev_scores_lda, axis=0)], axis=1))[:, 1]    print('Results on development set:')    # top-S EER    print('Modified baseline model:')    dev_EER = calculate_EER(dev_trials, np.max(dev_scores, axis=0))    print('LDA-based model:')    dev_EER = calculate_EER(dev_trials, np.max(dev_scores_lda, axis=0))    print('PLDA-based model:')    dev_EER = calculate_EER(dev_trials, np.max(dev_scores_plda, axis=0))    print('Ensemble:')    dev_EER = calculate_EER(dev_trials, dev_scores_fused)    # top-1 detector EER    dev_trials_confusion = get_trials_label_with_confusion(dev_identified_label, dev_trials_label, dev2train,                                                           dev_trials)    dev_EER_confusion = calculate_EER_with_confusion(dev_scores_fused, dev_trials_confusion)    ####################################################################################################################    # Repeat for test set    ####################################################################################################################    # renaming for submission    trn_bl_id = np.concatenate([trn_bl_id, dev_bl_id], axis=0)    trn_bg_id = np.concatenate([trn_bg_id, dev_bg_id], axis=0)    trn_bl_ivector = np.concatenate([trn_bl_ivector, dev_bl_ivector], axis=0)    trn_bg_ivector = np.concatenate([trn_bg_ivector, dev_bg_ivector], axis=0)    # encode labels    le = LabelEncoder()    trn_id_enc = le.fit_transform(trn_bl_id)    num_bl_spk = len(np.unique(trn_id_enc))    trn_id_cat = np_utils.to_categorical(trn_id_enc, num_classes=num_bl_spk)    dev_id_cat = np_utils.to_categorical(le.transform(dev_bl_id), num_classes=num_bl_spk)    # length normalization    print('normalizing lengths')    trn_bl_ivector_ln = length_norm(trn_bl_ivector)    trn_bg_ivector_ln = length_norm(trn_bg_ivector)    tst_ivector_ln = length_norm(tst_ivector)    # Calculating speaker mean vector    print('evaluating cosing similarity with M-Norm')    spk_mean, spk_mean_label = make_spkvec(trn_bl_ivector_ln, trn_bl_id)    scores = spk_mean.dot(tst_ivector_ln.transpose())    blscores = spk_mean.dot(trn_bl_ivector_ln.transpose())    mnorm_mu_bl = np.mean(blscores, axis=1)    mnorm_std_bl = np.std(blscores, axis=1)    for iter in range(np.shape(scores)[1]):        scores[:, iter] = (scores[:, iter]-mnorm_mu_bl)# / mnorm_std_bl    scores_bl = np.max(scores, axis=0)    # apply LDA for PLDA    print('Applying Linear Discriminant Analysis')    clf_deep = LinearDiscriminantAnalysis(n_components=600)    clf_deep.fit(trn_bl_ivector, trn_id_enc)    trn_bl_ivector_lda = clf_deep.transform(trn_bl_ivector)    tst_ivector_lda = clf_deep.transform(tst_ivector)    trn_bl_ivector_lda = length_norm(trn_bl_ivector_lda)    tst_ivector_lda = length_norm(tst_ivector_lda)    spk_mean_lda, spk_mean_label = make_spkvec(trn_bl_ivector_lda, trn_bl_id)    # evalute plda    print('Training plda model')    numiter = 20    vdim = 600    udim = 600    plda_model_path = 'plda_model_' + str(numiter) + '_' + str(vdim) + '_' + str(udim) + '.npy'    plda_model = plda.fit_plda_model(trn_bl_ivector_lda, trn_id_cat, numiter=numiter, vdim=vdim, udim=udim, rcond=1e-3)    scores_plda = plda.get_plda_scores(plda_model, spk_mean_lda, tst_ivector_lda)    scores_plda = (scores_plda - dev_min_plda) / (dev_max_plda - dev_min_plda)    tst_identified_label = spk_mean_label[(np.argmax(scores_plda, axis=0))]    # apply LDA for blacklist speakers    print('Applying Linear Discriminant Analysis')    clf_deep = LinearDiscriminantAnalysis(n_components=200)    clf_deep.fit(trn_bl_ivector, trn_id_enc)    trn_bl_ivector_lda = clf_deep.transform(trn_bl_ivector)    tst_ivector_lda = clf_deep.transform(tst_ivector)    trn_bl_ivector_lda = length_norm(trn_bl_ivector_lda)    tst_ivector_lda = length_norm(tst_ivector_lda)    spk_mean_lda, spk_mean_label = make_spkvec(trn_bl_ivector_lda, trn_bl_id)    scores_lda = spk_mean_lda.dot(tst_ivector_lda.transpose())    tst_scores_fused = clf.predict_proba(        np.stack([np.max(scores, axis=0), np.mean(scores, axis=0),                  np.max(scores_lda, axis=0), np.mean(scores_lda, axis=0),                  np.max(scores_plda, axis=0), np.mean(scores_plda, axis=0)], axis=1))[:, 1]    print('Results on test set:')    # top-S EER    print('Modified baseline model:')    dev_EER = calculate_EER(tst_trials, np.max(scores, axis=0))    print('LDA-based model:')    dev_EER = calculate_EER(tst_trials, np.max(scores_lda, axis=0))    print('PLDA-based model:')    dev_EER = calculate_EER(tst_trials, np.max(scores_plda, axis=0))    print('Ensemble:')    tst_EER = calculate_EER(tst_trials, tst_scores_fused)    # top-1 detector EER    tst_trials_confusion = get_trials_label_with_confusion(tst_identified_label, tst_trials_label, test2train,                                                           tst_trials)    tst_EER_confusion = calculate_EER_with_confusion(tst_scores_fused, tst_trials_confusion)    ####################################################################################################################    # Generate Submission File    ####################################################################################################################    filename = 'Fraunhofer FKIE_fixed_primary.csv'    # filename = 'Fraunhofer FKIE_fixed_contrastive1.csv'    with open(filename, "w") as text_file:        for iter, score in enumerate(tst_scores_fused):            id_in_trainset = tst_identified_label[iter].split('_')[0]            input_file = tst_utt[iter]            text_file.write('%s,%s,%s\n' % (input_file, score, train2id[id_in_trainset]))